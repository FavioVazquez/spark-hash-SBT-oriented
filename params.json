{"name":"Spark-hash-SBT-oriented","tagline":"This is my version of mrsqueeze's spark-hash but SBT oriented, all the credit of code and logic goes to him. This uses the sbt environment instead of the maven POM's environment","body":"Disclaimer\r\n==============================\r\nThis is my version of mrsqueeze's spark-hash but SBT oriented, all the credit of code and logic goes to him. This uses the sbt environment instead of the maven's environment.\r\nThe original repo is in [@mrsqueeze](https://github.com/mrsqueeze/) github, https://github.com/mrsqueeze/spark-hash\r\n\r\n**Note** = In Globals.scala you should change the default domains to your\r\nmaster spark domain (if using mesos) and your HDFS master domain. \r\n\r\nspark-hash\r\n==============================\r\n\r\nLocality sensitive hashing for [Apache Spark](http://spark.apache.org/).\r\nThis implementation was largely based on the algorithm described in chapter 3 of [Mining of Massive Datasets](http://mmds.org/) with some modifications for use in spark.\r\n\r\nSBT environment\r\n==============================\r\n\r\nThe steps to use this repo with SBT are the following:\r\n\r\n1.- Make sure you have the latest sbt installed. For Ubuntu and other Debian-based distributions:\r\n\r\nUbuntu and other Debian-based distributions use the DEB format, but usually you don’t install\r\nyour software from a local DEB file. Instead they come with package managers both for the \r\ncommand line (e.g. apt-get, aptitude) or with a graphical user interface (e.g. Synaptic).\r\nRun the following from the terminal to install sbt (You’ll need superuser privileges to do so, \r\nhence the sudo).\r\n\r\n\techo \"deb http://dl.bintray.com/sbt/debian /\" | sudo tee -a /etc/apt/sources.list.d/sbt.list\r\n\tsudo apt-get update\r\n\tsudo apt-get install sbt\r\n\t\r\n2.- Add the Spark JAR to your project. To do this copy the JAR from your local Spark folder to\r\na folder in the project main tree. By example (assuming your local spark is in /opt and your\r\nproject is in Documents/myproject):\r\n\r\n\tsudo cp /opt/spark/lib/Spark.jar /home/username/Documents/myproject/lib\r\n\t\r\n3.- In the project's folder run SBT assembly\r\n\r\n\tsbt assembly\r\n\t\r\n4.- I strongly recommend that you upload the data folder to HDFS if you want to use the \r\nexample given data in cluster mode, or you'll going to have to copy the project folder to the other datanodes. \r\nTo do so use (to place it in the root folder):\r\n\r\n\thadoop fs -put data/ /\r\n\t\r\n5.- If you are using Spark in cluster (with Mesos) mode you can use spark-submit to run the code (in the Globals is defined  a generic domain for Spark Master using Apache Mesos and and also a generic HDFS domain, please change the masters according to your personalized domain): \r\n\r\n\tspark-submit --class=\"package.classname\" target/scala-2.10/spark-hash.jar\r\n\t\r\n6.- To run it in local mode, just comment the setMaster(Globals.masterSpark) and uncomment the \r\nset.Master(\"local\") line.\r\n\r\n- If you want tu use the default Spark distribution in the maven repositories just add to the \r\nbuild.sbt the following lines in the libraryDependencies section, with provided if you have\r\nuploaded your spark dist to HDFS, and without it if you have not:\r\n\r\n\t\"org.apache.spark\" % \"spark-core_2.10\" % \"1.3.1\" % \"provided\"\r\n\t\r\n\t\"org.apache.spark\" %% \"spark-sql\"  % \"1.3.1\" % \"provided\"\r\n\r\nExample Data\r\n-----\r\nWe executed some nmap probes and now would like to group IP addresses with similar sets of \r\nexposed ports. An small example of this dataset has been provided in data/sample.dat for \r\nillustration. \r\n\r\nAs part of the preprocessing which was performed on nmap xml files, we flattened IP addresses\r\nwith identical port sets. The flattening did not factor the time when the port was open. \r\n\r\nTo run SparkHashTry you have to specify the class in the spark-submit (if running in cluster mode)\r\n\r\n\tspark-submit --class=\"spark.hash.SparkHashTry\" target/scala-2.10/spark-hash.jar\r\n\t\r\n\r\nIn the SparkHashTry example we explore the data, and later we will use the Hasher and LSH to\r\nmake use of the great mrsqueeze code.\r\n\t\r\n\tval port_set: RDD[(List[Int],Int)] = sc.objectFile(Globals.masterHDFS+\"/data/sample.dat\")\r\n\tport_set.take(5).foreach(println)\r\n\t\r\nResult:\r\n\r\n\t(List(21, 23, 443, 8000, 8080),1)\r\n\t(List(80, 3389, 49152, 49153, 49154, 49155, 49156, 49157),9)\r\n\t(List(21, 23, 80, 2000, 3000),13)\r\n\t(List(1723),1)\r\n\t(List(3389),1)\r\n\r\nEach row in the RDD is a Tuple2 containing the open ports alone with the number of distinct IP addresses that had those ports. In the above example, 13 IP addresses had ports 21, 23, 80, 2000, 3000 opened.\r\n\r\nCount the RDD for the total dataset size:\r\n\t\r\n\t  println(port_set.count())\r\n\t \r\nResult:\r\n\r\n\t261\r\n\t\r\nCount the total number of IP addresses that contributed to this dataset:\r\n\r\n\tprintln(port_set.map(x => x._2).reduce(_ + _))\r\n\r\nResult:\r\n\t\r\n\t2273\r\n\r\nShow the top five port sets sorted by IP count:\r\n\r\n\tport_set.sortBy(_._2, false).take(5).foreach(println)\r\n\t\r\nResult:\r\n\t\r\n\t(List(21, 23, 80),496)\r\n\t(List(22, 53, 80),289)\r\n\t(List(80, 443),271)\r\n\t(List(80),228)\r\n\t(List(22, 53, 80, 443),186)\r\n\r\nFilter the dataset to drop port sets smaller than 2. Show the top 5 results:\r\n\r\n\tval port_set_filtered = port_set.filter(tpl => tpl._1.size >= 2)\r\n\tport_set_filtered.sortBy(_._2, false).take(5).foreach(println)\r\n\t\r\nResult:\r\n\r\n\t(List(21, 23, 80),496)\r\n\t(List(22, 53, 80),289)\r\n\t(List(80, 443),271)\r\n\t(List(22, 53, 80, 443),186)\r\n\t(List(21, 22, 23, 80, 2000),73)\r\n\r\nShow the top three port sets by set size:\r\n\r\n\tport_set.sortBy(_._1.size, false).take(3).foreach(println)\r\n\t\r\nResult:\r\n\t\r\n\t(List(13, 21, 37, 80, 113, 443, 1025, 1026, 1027, 2121, 3000, 5000, 5101, 5631),1)\r\n\t(List(21, 80, 443, 1723, 3306, 5800, 5900, 8081, 49152, 49153, 49154, 49155),1)\r\n\t(List(21, 22, 26, 80, 110, 143, 443, 465, 587, 993, 995, 3306),3)\r\n\r\n\r\nImplementation Details\r\n----\r\n\r\nImplementation of LSH follows the rough steps\r\n\r\n1. minhash each vector some number of times. The number of times to hash is an input parameter.\r\n The hashing function is defined in spark.hash.Hasher. Essentially each element of the input vector is \r\n hashed and the minimum hash value for the vector is returned. Minhashing produces a set of \r\n signatures for each vector.\r\n2. Chop up each vector's minhash signatures into bands where each band contains an equal \r\nnumber of signatures. Bands with a greater number of signatures will produce clusters with \r\n*greater* similarity. A greater number of bands will increase the probabilty that similar \r\nvector signatures  hash to the same value.\r\n3. Order each of the vector bands such that for each band the vector's data for that\r\n band are grouped together. \r\n4. Hash each band and group similar values. These similar values for a given band \r\nthat hash to the same value are added to the result set.\r\n5. Optionally filter results. An example operation would be to filter out  singleton sets.\r\n\r\n#### Example\r\n\r\nInput data\r\n\r\n1. [21, 25, 80, 110, 143, 443]\r\n2. [21, 25, 80, 110, 143, 443, 8080]\r\n3. [80, 2121, 3306, 3389, 8080, 8443]\r\n4. [13, 17, 21, 23, 80, 137, 443, 3306, 3389]\r\n\r\nLet's hash each vector 1000 times. To do this we'll need to create 1000 hash functions and\r\n minhash each vector 1000 times.\r\n\r\n1. 1000 minhash signatures\r\n2. 1000 minhash signatures\r\n3. 1000 minhash signatures\r\n4. 1000 minhash signatures\r\n\r\nNow we want to chop up the signatures into 100 bands where each band will have 10 elements.\r\n\r\n1. band 1 (10 elements), band 2 (10 elements), ... , band 100 (10 elements)\r\n2. band 1 (10 elements), band 2 (10 elements), ... , band 100 (10 elements)\r\n3. band 1 (10 elements), band 2 (10 elements), ... , band 100 (10 elements)\r\n4. band 1 (10 elements), band 2 (10 elements), ... , band 100 (10 elements)\r\n\r\nFor each of the 4 sets of bands, group all of band 1, band 2, .... band 4\r\n\r\n\tband 1: vector 1 (10 elements), ... , vector 4 (10 elements) \r\n\r\n\tband 2: vector 1 (10 elements), ... , vector 4 (10 elements) \r\n\t\r\n\tband 100: vector 1 (10 elements), ... , vector 4 (10 elements) \r\n\r\nFor each band, hash each of 10 element signatures.\r\n\r\n\tband 1: 10, 10, 3, 4\r\n\r\n\tband 2: 6, 5, 2, 4\r\n\t\r\n\tband 100: 1, 2, 3, 8\r\n\r\nGroup identical values within each band. This correspond to the similar clusters. In the above example, only two vectors (1 and 2) are deemed similar.\r\n\r\n\tmodel.clusters.foreach(println)\r\n\t(0,CompactBuffer((65535,[21,25,80,110,143,443],[1.0,1.0,1.0,1.0,1.0,1.0]), (65535,[21,25,80,110,143,443,8080],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])))\r\n\r\n\r\nUsage\r\n-----\r\n\r\nThe previously described data can easily be converted into Spark's SparseVector class with the open port corresponding to an index in the vector. With that in mind, there are a few domain specific tunables that will need to be set prior to running LSH\r\n\r\n- *p* - a prime number > the largest vector index. In the case of open ports, this number is set to 65537.\r\n\r\n- *m* - the number of \"bins\" to hash data into. Smaller numbers increase collisions. We use 1000.\r\n\r\n- *numRows* - the total number of times to minhash a vector. *numRows* separate hash functions are generated. Larger numbers produce more samples and increase the likelihood similar vectors will hash together. \r\n\r\n- *numBands* - how many times to chop *numRows*. Each band will have *numRows*/*numBand* hash signatures. The larger number of elements the higher confidence in vector similarity. \r\n\r\n- *minClusterSize* - a post processing filter function that excludes clusters below a threshold.\r\n\r\n\r\n### Driver Class\r\n\r\nExecuting a driver in local mode. This executes spark.hash.OpenPortDriver and saves the\r\nresulting cluster output to the file results.csv. In this case 80% of the data is sampled\r\nand spread over 8 partitions. For normal use the driver will need to be modified to handle\r\ndata load, requisite transforms, and parameter tuning. Also, for some datasets it may not be \r\npractical to save all results to a local driver.\r\n\r\n- To run in local mode comment the setMaster(Globals.masterSpark) line and uncomment the \r\nsetMaster(\"local\"). To run in cluster mode, do the opposite (cluster mode is the default)\r\n\r\n\t\tspark-submit --class=\"spark.hash.OpenPortApp\" target/scala-2.10/spark-hash.jar hdfs://yourdomain.com:8020/data/sample.dat\r\n\r\nResult:\r\n\r\n\tUsage: OpenPortApp <file> <partitions> <data_sample>\r\n\r\n- To sample (80%) and spread over 8 partitions:\r\n\r\n\t\tspark-submit --class=\"spark.hash.OpenPortApp\" target/scala-2.10/spark-hash.jar hdfs://yourdomain.com:8020/data/sample.dat 8 0.5\r\n\r\n\r\n### Finding similar sets for a new point\r\n\r\nThis is implemented in the JaccardTry.scala file, to run:\r\n\r\n\tspark-submit --class=\"spark.hash.JaccardTry\" target/scala-2.10/spark-hash.jar \r\n\r\n---\r\n\r\n\tval np = List(21, 23, 80, 2000, 8443)\r\n\tval nv = Vectors.sparse(65535, np.map(x => (x, 1.0))).asInstanceOf[SparseVector]\r\n\t\r\n\t//use jaccard score of 0.50 across the entire cluster. This may be a bit harsh for large tests.\r\n\tval sim = lsh.compute(nv, model, 0.50)\r\n\t\r\n\tprintln(sim.count())\r\n\t\r\n\tsim.collect().foreach(println)\r\n---\r\n\r\nResult:\r\n\t\r\n\t(9,List((65535,[21,22,23,80,2000,3389,8000],[1.0,1.0,1.0,1.0,1.0,1.0,1.0]), (65535,[21,22,23,80,2000,3389],[1.0,1.0,1.0,1.0,1.0,1.0]), (65535,[21,23,80,2000,8443],[1.0,1.0,1.0,1.0,1.0])))\r\n\t(4,List((65535,[21,23,80,81,2000],[1.0,1.0,1.0,1.0,1.0]), (65535,[21,23,80,2000,8081],[1.0,1.0,1.0,1.0,1.0]), (65535,[21,23,80,2000,8443],[1.0,1.0,1.0,1.0,1.0])))\r\n\t(5,List((65535,[21,22,23,80,1723,2000,8000],[1.0,1.0,1.0,1.0,1.0,1.0,1.0]), (65535,[21,22,23,80,1723,2000],[1.0,1.0,1.0,1.0,1.0,1.0]), (65535,[21,23,80,2000,8443],[1.0,1.0,1.0,1.0,1.0])))\r\n\t(6,List((65535,[21,22,23,53,80,2000,8000],[1.0,1.0,1.0,1.0,1.0,1.0,1.0]), (65535,[21,22,23,53,80,2000],[1.0,1.0,1.0,1.0,1.0,1.0]), (65535,[21,23,80,2000,8443],[1.0,1.0,1.0,1.0,1.0])))\r\n\t(7,List((65535,[21,22,23,80,554,2000],[1.0,1.0,1.0,1.0,1.0,1.0]), (65535,[21,22,23,80,554,2000,8000],[1.0,1.0,1.0,1.0,1.0,1.0,1.0]), (65535,[21,23,80,2000,8443],[1.0,1.0,1.0,1.0,1.0])))\r\n\r\n\r\n### Tuning Results\r\nAs described in the MMDS book, LSH can be tuned by adjusting the number of rows and bands such that:\r\n\r\n\tthreshold = Math.pow(1/bands),(1/(rows/bands))\r\n\t\r\nNaturally, the number of rows, bands, and the resulting size of the band (rows/bands) dictates the quality of results yielded by LSH. Higher thresholds produces clusters with higher similarity. Lower thresholds typically produce more clusters but sacrifices similarity. \r\n\r\nRegardless of parameters, it may be good to independently verify each cluster. One such verification method is to calculate the jaccard similarity of the cluster (it is a set of sets). Implementation of jaccard similarity is provided in LSH.jaccard.\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}